2467.16+2.14+119.3+5.71+17.69
2467.16+2.14
2467.16+2.14+119.3
2467.16+2.14+119.3+5.71
2612-2467.16
2612-2469.3
2612-2588.6
2612-2594.31
17.69+5.71
10*log(2612/10)+2
log(261.2)
log(261.2)*10
10*log(144.84/10)+2*2
10*log(142.7/10)+2*3
〖AIC〗_4=10*log(23.4/10)+2*4
10*log(23.4/10)+2*4
10*log(17.69/10)+2*5
10*log(23.4/10)+2*4
10*log(17.69/10)+2*5
17.69/5
2612/3.54-(10-2)
2612/3.54
737.8531-8
144.84/3.54-(10-2*2)
142.7/3.54-(10-2*3)
23.4/3.54-(10-2*4)
17.69/3.54-(10-2*5)
1.92/(1-0.44)
1-0.44
1.92/0.56
0.7/(1-0.71)
0.14/(1-0.25)
(-2.37)/(1-0.52)
(-0.01)/(1-0.12)
(-1.46)/(1-0.33)
0.75/(1-0.51)
(-1.1)/(1-0.38)
1.92/(1-0.24)
(-0.5)/(1-0.5)
x<-3.428571^2+2.413793^2+0.1866667^2+-4.9375^2
-4.9375^2
x<-3.428571^2+2.413793^2+0.1866667^2+(-4.9375)^2+(-0.01136364)^2
(-4.9375)^2
x<-3.428571^2+2.413793^2+0.1866667^2+(-4.9375)^2+(-0.01136364)^2+(-2.179104)^2+1.530612^2+(-1.774194)^2+2.526316^2+1
x
2.3-3.5
1.7-3.5
1.2-1.6
0.9-1.6
15+10+20
106.984-75.787
106.984-67.442
31+6.5+1.75
1-pchisq(39.542,4)
exp(1.33)
8.95918^2
80.27+64.38+2*-15.47+9*58.78+6*-3.28+6*8.28
(-47.17882-1.07436+3*16.96087)/√672.73
(-47.17882-1.07436+3*16.96087)/sqrt(672.73)
-47.17882-1.07436+3*16.96087
sqrt(672.73)
1-pt(abs(0.1013774),55)
2*(1-pt(abs(0.1013774),55))
0.90565-1.07436+10.82241*0.02+16.96087*0.1
ex pa(1.743825)
exp((1.743825)
exp(1.743825)
69.249-67.442
0.0566+1.7506
1-pchisq(1.807,2)
67.442/55
sqrt(0.27)
sqrt(0.28)
0.7004/0.5196152
(-0.5881)/0.5291503
1.1843/0.5291503
1-pt(1.347921,27)
2*(1-pt(1.347921,27))
2*(1-pt(1.111404,27))
2*(1-pt(2.238116,27))
41.054-33.672
1-pchisq(7.382,2)
30/100
0.7004-0.5881*0.3+1.1843*0.5
0.3^2*0.28+0.5^2*0.28+2*0.3*0.5*-0.04
0.09*0.28+0.25*0.28+0.6*0.5*-0.04
qnorm(0.1)
qnorm(0.25)
qnorm(0.025)
qnorm(0.05)
sqrt(0.0832)
1.11612-1.64*0.2884441
1.11612+1.64*0.2884441
1/(1/0.6430717+1)
1/0.6430717
1/2.555
1/(1/1.589168+1)
1-0.3913894
1-0.6137755
30-17
10/17
9/13
17*13
13+5+11+6+13+13+5+13+13+6+6+7+13+7+13+13+13
8+2+7+8+7+7+6+6
17*13
221-170-51
170/221
(-47.17882-1.07436+3*16.96087)/sqrt(672.73)
2*(1-pnorm(0.1013774,0,1))
2*(1-pnorm(0.1013774))
2(1-pnorm(1.347921))
2*(1-pnorm(1.347921))
2*(1-pnorm(1.111404))
2*(1-pnorm(2.238116))
0.6430717/(1+0.6430717)
pnorm(2.698)
1-pnorm(2.698)
1-0.003487872*2
ppoints(3)
1-0.5/3
0.5/3
qnorm(0.05)
qnorm(0.025)
#这个是wilcoxon signed rank test，有tied所以有warning
wilcox.test(stayhome,goabroad,alternative = "two.sided")
#####Q3
###a
twins<-read.csv("~/Desktop/5505/homework10/xid-48104659_1.dat",sep="")
stayhome<-twins$Sample1
goabroad<-twins$Sample2
#这个是wilcoxon signed rank test，有tied所以有warning
wilcox.test(stayhome,goabroad,alternative = "two.sided")
#这个是wilcoxon signed rank test，有tied所以有warning
wilcox.test(stayhome,goabroad,alternative = "two.sided",correct=FALSE)
vector("list",2)
matrix(0,10,2)
library(MASS)
source('~/Desktop/Machine Learning/ML in Python and R/Dimensionality Reduction/PCA/PCA.R')
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("PC1","PC2")
y_grid<-predict(classifier,grid_set)
plot(set[,-3],main="CLassifier (Training set)",xlab="PC1",ylab="PC2",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==2,"Deepskyblue",ifelse(y_grid==1,"red","green")))
points(set[,-3],pch=21,bg=ifelse(set[,3]==2,"black",ifelse(set[,3]==1,"orange","blue")))
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Dimensionality Reduction/Kernel PCA")
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#Feature Scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting logistic regression to training set
classifier<-glm(Purchased~.,family=binomial,data=training_set)
summary(classifier)
#Predicting the test set results
prob_pred<-predict(classifier,type="response",test_set[-3])
y_pred<-ifelse(prob_pred>0.5,1,0)
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Visualizing the training set results
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("Age","EstimatedSalary")
prob_set<-predict(classifier,type = "response",grid_set)
y_grid<-ifelse(prob_set>0.5,1,0)
plot(set[,-3],main="CLassifier (Training set)",xlab="Age",ylab="EstimatedSalary",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Logistic")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#Feature Scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting logistic regression to training set
classifier<-glm(Purchased~.,family=binomial,data=training_set)
summary(classifier)
#Predicting the test set results
prob_pred<-predict(classifier,type="response",test_set[-3])
y_pred<-ifelse(prob_pred>0.5,1,0)
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Visualizing the training set results
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("Age","EstimatedSalary")
prob_set<-predict(classifier,type = "response",grid_set)
y_grid<-ifelse(prob_set>0.5,1,0)
plot(set[,-3],main="CLassifier (Training set)",xlab="Age",ylab="EstimatedSalary",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Dimensionality Reduction/Kernel PCA")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#Feature Scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting logistic regression to training set
classifier<-glm(Purchased~.,family=binomial,data=training_set)
summary(classifier)
#Predicting the test set results
prob_pred<-predict(classifier,type="response",test_set[-3])
y_pred<-ifelse(prob_pred>0.5,1,0)
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Visualizing the training set results
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("Age","EstimatedSalary")
prob_set<-predict(classifier,type = "response",grid_set)
y_grid<-ifelse(prob_set>0.5,1,0)
plot(set[,-3],main="CLassifier (Training set)",xlab="Age",ylab="EstimatedSalary",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
install.packages("kernlab")
#Applying Kernel PCA
library(kernlab)
View(training_set)
kpca<-kpca(~.,data=training_set[,-3],kernel="rbfdot",features=2)
training_set_pca<-predict(kpca,training_set)
View(training_set_pca)
View(training_set)
training_set_pca<-as.data.frame(predict(kpca,training_set))
View(training_set_pca)
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Dimensionality Reduction/Kernel PCA")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#Feature Scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Applying Kernel PCA
library(kernlab)
kpca<-kpca(~.,data=training_set[,-3],kernel="rbfdot",features=2)
training_set_pca<-as.data.frame(predict(kpca,training_set))
training_set_pca$Purchased<-training_set$Purchased
test_set_pca<-as.data.frame(predict(kpca,test_set))
test_set_pca$Purchased<-test_set$Purchased
training_set<-training_set_pca
test_set<-test_set_pca
#Fitting logistic regression to training set
classifier<-glm(Purchased~.,family=binomial,data=training_set)
summary(classifier)
#Predicting the test set results
prob_pred<-predict(classifier,type="response",test_set[-3])
y_pred<-ifelse(prob_pred>0.5,1,0)
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Visualizing the training set results
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("Age","EstimatedSalary")
prob_set<-predict(classifier,type = "response",grid_set)
y_grid<-ifelse(prob_set>0.5,1,0)
plot(set[,-3],main="CLassifier (Training set)",xlab="Age",ylab="EstimatedSalary",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
cm
cm
#Visualizing the training set results
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("V1","V2")
prob_set<-predict(classifier,type = "response",grid_set)
y_grid<-ifelse(prob_set>0.5,1,0)
plot(set[,-3],main="CLassifier (Training set)",xlab="V1",ylab="V2",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Dimensionality Reduction/Kernel PCA")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Applying Kernel PCA
library(kernlab)
kpca<-kpca(~.,data=training_set[,-3],kernel="rbfdot",features=2)
training_set_pca<-as.data.frame(predict(kpca,training_set))
training_set_pca$Purchased<-training_set$Purchased
test_set_pca<-as.data.frame(predict(kpca,test_set))
test_set_pca$Purchased<-test_set$Purchased
training_set<-training_set_pca
test_set<-test_set_pca
classifier<-glm(Purchased~.,family=binomial,data=training_set)
summary(classifier)
#Predicting the test set results
prob_pred<-predict(classifier,type="response",test_set[-3])
y_pred<-ifelse(prob_pred>0.5,1,0)
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
cm
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("V1","V2")
prob_set<-predict(classifier,type = "response",grid_set)
y_grid<-ifelse(prob_set>0.5,1,0)
plot(set[,-3],main="CLassifier (Training set)",xlab="V1",ylab="V2",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
View(training_set_pca)
View(training_set)
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Model Selection")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Model Selection")
#Applying k-Fold cross validation
library(caret)
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Model Selection")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
View(dataset)
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Model Selection")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#Feature Scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting SVM to training set
library(e1071)
classifier<-svm(Purchased~.,data=training_set,type="C-classification",kernel="radial")
summary(classifier)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Applying k-Fold cross validation
library(caret)
dataset[,c(1,2)]<- scale(dataset[,c(1,2)], center = mins, scale = maxs - mins)
folds<-createFolds(dataset$Purchased,k=10)#把数据分成10块
folds
View(folds)
library(caret)
dataset[,c(1,2)]<- scale(dataset[,c(1,2)], center = mins, scale = maxs - mins)
folds<-createFolds(dataset$Purchased,k=10)#把数据分成10块
cv<-lapply(folds,function(x){
training_fold<-dataset[-x,]
test_fold<-dataset[x,]
classifier<-svm(Purchased~.,data=training_fold,type="C-classification",kernel="radial")
y_pred<-predict(classifier,test_fold[,-3])
cm <-table(test_fold$Purchased,y_pred)
accuracy<-(cm[1,1]+cm[2,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
return(accuracy)
})
cv
mean(as.numeric(cv))
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Model Selection")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#Feature Scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting SVM to training set
library(e1071)
classifier<-svm(Purchased~.,data=training_set,type="C-classification",kernel="radial")
summary(classifier)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Applying grid search to find the best parameters
library(caret)
classifier<-train(Purchased~.,data=training_set,method="svmRadial")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Model Selection")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#Feature Scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting SVM to training set
library(e1071)
classifier<-svm(Purchased~.,data=training_set,type="C-classification",kernel="radial")
summary(classifier)
classifier<-train(Purchased~.,data=training_set,method="svmRadial")
str(training_set)
#Applying grid search to find the best parameters
library(caret)
training_set$Purchased<-factor(training_set$Purchased)
classifier<-train(Purchased~.,data=training_set,method="svmRadial")
View(classifier)
classifier
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Model Selection")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#Feature Scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting SVM to training set
library(e1071)
classifier<-svm(Purchased~.,data=training_set,type="C-classification",kernel="radial")
summary(classifier)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Applying grid search to find the best parameters
library(caret)
training_set$Purchased<-factor(training_set$Purchased)
grid_search<-train(Purchased~.,data=training_set,method="svmRadial")
grid_search
grid_search$bestTune
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Model Selection")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#Feature Scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting SVM to training set
library(e1071)
classifier<-svm(Purchased~.,data=training_set,type="C-classification",kernel="radial")
summary(classifier)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Applying grid search to find the best parameters
library(caret)
training_set$Purchased<-factor(training_set$Purchased)
grid_search<-train(Purchased~.,data=training_set,method="svmRadial")
grid_search
grid_search$bestTune
grid_search
grid_search$bestTune
