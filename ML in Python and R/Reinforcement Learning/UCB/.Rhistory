2612-2467.16
2612-2469.3
2612-2588.6
2612-2594.31
17.69+5.71
10*log(2612/10)+2
log(261.2)
log(261.2)*10
10*log(144.84/10)+2*2
10*log(142.7/10)+2*3
〖AIC〗_4=10*log(23.4/10)+2*4
10*log(23.4/10)+2*4
10*log(17.69/10)+2*5
10*log(23.4/10)+2*4
10*log(17.69/10)+2*5
17.69/5
2612/3.54-(10-2)
2612/3.54
737.8531-8
144.84/3.54-(10-2*2)
142.7/3.54-(10-2*3)
23.4/3.54-(10-2*4)
17.69/3.54-(10-2*5)
1.92/(1-0.44)
1-0.44
1.92/0.56
0.7/(1-0.71)
0.14/(1-0.25)
(-2.37)/(1-0.52)
(-0.01)/(1-0.12)
(-1.46)/(1-0.33)
0.75/(1-0.51)
(-1.1)/(1-0.38)
1.92/(1-0.24)
(-0.5)/(1-0.5)
x<-3.428571^2+2.413793^2+0.1866667^2+-4.9375^2
-4.9375^2
x<-3.428571^2+2.413793^2+0.1866667^2+(-4.9375)^2+(-0.01136364)^2
(-4.9375)^2
x<-3.428571^2+2.413793^2+0.1866667^2+(-4.9375)^2+(-0.01136364)^2+(-2.179104)^2+1.530612^2+(-1.774194)^2+2.526316^2+1
x
2.3-3.5
1.7-3.5
1.2-1.6
0.9-1.6
15+10+20
106.984-75.787
106.984-67.442
31+6.5+1.75
1-pchisq(39.542,4)
exp(1.33)
8.95918^2
80.27+64.38+2*-15.47+9*58.78+6*-3.28+6*8.28
(-47.17882-1.07436+3*16.96087)/√672.73
(-47.17882-1.07436+3*16.96087)/sqrt(672.73)
-47.17882-1.07436+3*16.96087
sqrt(672.73)
1-pt(abs(0.1013774),55)
2*(1-pt(abs(0.1013774),55))
0.90565-1.07436+10.82241*0.02+16.96087*0.1
ex pa(1.743825)
exp((1.743825)
exp(1.743825)
69.249-67.442
0.0566+1.7506
1-pchisq(1.807,2)
67.442/55
sqrt(0.27)
sqrt(0.28)
0.7004/0.5196152
(-0.5881)/0.5291503
1.1843/0.5291503
1-pt(1.347921,27)
2*(1-pt(1.347921,27))
2*(1-pt(1.111404,27))
2*(1-pt(2.238116,27))
41.054-33.672
1-pchisq(7.382,2)
30/100
0.7004-0.5881*0.3+1.1843*0.5
0.3^2*0.28+0.5^2*0.28+2*0.3*0.5*-0.04
0.09*0.28+0.25*0.28+0.6*0.5*-0.04
qnorm(0.1)
qnorm(0.25)
qnorm(0.025)
qnorm(0.05)
sqrt(0.0832)
1.11612-1.64*0.2884441
1.11612+1.64*0.2884441
1/(1/0.6430717+1)
1/0.6430717
1/2.555
1/(1/1.589168+1)
1-0.3913894
1-0.6137755
30-17
10/17
9/13
17*13
13+5+11+6+13+13+5+13+13+6+6+7+13+7+13+13+13
8+2+7+8+7+7+6+6
17*13
221-170-51
170/221
(-47.17882-1.07436+3*16.96087)/sqrt(672.73)
2*(1-pnorm(0.1013774,0,1))
2*(1-pnorm(0.1013774))
2(1-pnorm(1.347921))
2*(1-pnorm(1.347921))
2*(1-pnorm(1.111404))
2*(1-pnorm(2.238116))
0.6430717/(1+0.6430717)
pnorm(2.698)
1-pnorm(2.698)
1-0.003487872*2
ppoints(3)
1-0.5/3
0.5/3
qnorm(0.05)
qnorm(0.025)
#这个是wilcoxon signed rank test，有tied所以有warning
wilcox.test(stayhome,goabroad,alternative = "two.sided")
#####Q3
###a
twins<-read.csv("~/Desktop/5505/homework10/xid-48104659_1.dat",sep="")
stayhome<-twins$Sample1
goabroad<-twins$Sample2
#这个是wilcoxon signed rank test，有tied所以有warning
wilcox.test(stayhome,goabroad,alternative = "two.sided")
#这个是wilcoxon signed rank test，有tied所以有warning
wilcox.test(stayhome,goabroad,alternative = "two.sided",correct=FALSE)
vector("list",2)
matrix(0,10,2)
library(MASS)
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Decision Tree")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释决策树的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Decision Tree to training set
library(rpart)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
classifier<-rpart(Purchased~., data=training_set)
#在不做feature scaling时，画出决策树
#plot(classifier)
#text(classifier)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3],type = "class")
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Visualizing the training set results
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("Age","EstimatedSalary")
y_grid<-predict(classifier,grid_set,type = "class")
plot(set[,-3],main="CLassifier (Training set)",xlab="Age",ylab="EstimatedSalary",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
#比起python，并没有过度拟合
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Random Forest")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释随机森林的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(rpart)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
classifier<-rpart(Purchased~., data=training_set)
#在不做feature scaling时，画出决策树
#plot(classifier)
#text(classifier)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3],type = "class")
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Visualizing the training set results
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("Age","EstimatedSalary")
y_grid<-predict(classifier,grid_set,type = "class")
plot(set[,-3],main="CLassifier (Training set)",xlab="Age",ylab="EstimatedSalary",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
#比起python，并没有过度拟合
#Fitting Random Forest to training set
library(randomForest)
training_set[,-3]
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Random Forest")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离,我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(randomForest)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
set.seed(1234)
classifier<-randomForest(x = training_set[,-3],y = training_set[,3],ntree=10)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Visualizing the training set results
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("Age","EstimatedSalary")
y_grid<-predict(classifier,grid_set,type = "class")
plot(set[,-3],main="CLassifier (Training set)",xlab="Age",ylab="EstimatedSalary",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
View(dataset)
dataset<-dataset[,c(3,4)]
View(dataset)
dataset<-read.csv("Mall_Customers.csv")
dataset<-dataset[,c(4,5)]
View(dataset)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i)$tot.withinss
}
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#Using the elbow method to indentify the optimal #clusters
set.seed(6)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i)$tot.withinss
}
Cost
plot(c(1:10),Cost)
plot(c(1:10),Cost,type="b")
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#Using the elbow method to indentify the optimal #clusters
set.seed(6)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#Using the elbow method to indentify the optimal #clusters
set.seed(6)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#Using the elbow method to indentify the optimal #clusters
set.seed(7)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#Using the elbow method to indentify the optimal #clusters
set.seed(1234)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#Using the elbow method to indentify the optimal #clusters
set.seed(6)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=100,nstart=10)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#Using the elbow method to indentify the optimal #clusters
set.seed(6)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=500,nstart=10)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#Using the elbow method to indentify the optimal #clusters
set.seed(6)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=500,nstart=10)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#Using the elbow method to indentify the optimal #clusters
set.seed(6)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=500,nstart=20)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#Using the elbow method to indentify the optimal #clusters
set.seed(6)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=10,nstart=20)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#Using the elbow method to indentify the optimal #clusters
set.seed(6)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=1,nstart=20)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#Using the elbow method to indentify the optimal #clusters
set.seed(6)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=100,nstart=20)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
#Using the elbow method to indentify the optimal #clusters
set.seed()
#Using the elbow method to indentify the optimal #clusters
set.seed(NULL)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=100,nstart=20)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=100,nstart=20)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=100,nstart=20)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=100,nstart=20)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=100,nstart=20)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
#Fitting K-means to the dataset
rnorm(10)
#Fitting K-means to the dataset
rnorm(10)
#Fitting K-means to the dataset
rnorm(10)
#Using the elbow method to indentify the optimal #clusters
set.seed(6)
#Fitting K-means to the dataset
rnorm(10)
#Fitting K-means to the dataset
rnorm(10)
#Fitting K-means to the dataset
rnorm(10)
set.seed(6)
rnorm(10)
set.seed(6)
rnorm(10)
set.seed(6)
rnorm(10)
set.seed(6)
rnorm(10)
set.seed(6)
rnorm(10)
#Using the elbow method to indentify the optimal #clusters
set.seed(6)
rnorm(10)
rnorm(10)
rnorm(10)
#Fitting K-means to the dataset
kmeans = kmeans(X,5,iter.max = 300,nstart = 10)
#Fitting K-means to the dataset
kmeans<-kmeans(X,5,iter.max = 300,nstart = 10)
y_kmeans<-kmeans$cluster
y_kmeans
#Visualizing the clusters
library(cluster)
clusplot(X,y_kmeans)
clusplot(X,y_kmeans,lines = 0)
clusplot(X,y_kmeans,lines = 0,shade = TRUE,color = TRUE,labels = 2)
clusplot(X,y_kmeans,lines = 0,shade = TRUE,color = TRUE,labels = 2,plotchar = FALSE,span=TRUE)
clusplot(X,y_kmeans,lines = 0,shade = TRUE,color = TRUE,labels = 2,plotchar = FALSE)
clusplot(X,y_kmeans,lines = 0,shade = TRUE,color = TRUE,labels = 2,plotchar = FALSE,span = TRUE,
main="Clusters of customers",xlab = "Income",ylab = "Spending score")
View(X)
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Clustering")
dataset<-read.csv("Mall_Customers.csv")
X<-dataset[,c(4,5)]
#feature scaling
maxs <- apply(X, 2, max)
mins <- apply(X, 2, min)
X<- scale(X, center = mins, scale = maxs - mins)
set.seed(6)
Cost<-c()
for (i in 1:10){
Cost[i]<-kmeans(X,i,iter.max=300,nstart=20)$tot.withinss
}
plot(c(1:10),Cost,type="b",main="The elbow method",xlab = "#Clusters",ylab = "Cost")
#select 5 clusters
#Fitting K-means to the dataset
kmeans<-kmeans(X,5,iter.max = 300,nstart = 10)
y_kmeans<-kmeans$cluster
#Visualizing the clusters
library(cluster)
clusplot(X,y_kmeans,lines = 0,shade = TRUE,color = TRUE,labels = 2,plotchar = FALSE,span = TRUE,
main="Clusters of customers",xlab = "Income",ylab = "Spending score")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Association Rule Learning-Apriori")
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Association Rule Learning-Apriori")
dataset<-read.csv("Market_Basket_Optimisation.csv")
View(dataset)
dataset<-read.csv("Market_Basket_Optimisation.csv",header = FALSE)
View(dataset)
unique(dataset)
length(unique(dataset))
install.packages("arules")
library(arules)
dataset<-read.transactions("Market_Basket_Optimisation.csv",sep=",", rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset,topN=100)
itemFrequencyPlot(dataset,topN=50)#画出出现频率最高的前50个物品
21/7500
#Training Aprior on the dataset
rules<-apriori(dataset,parameter = list(support=0.003,confidence=0.8))
#Training Aprior on the dataset
rules<-apriori(dataset,parameter = list(support=0.003,confidence=0.4))
#Visualizing the results
inspect(rules[1:10])
#Visualizing the results
inspect(sort(rules,by="lift")[1:10])
#Training Aprior on the dataset
rules<-apriori(dataset,parameter = list(support=0.003,confidence=0.2))
#Visualizing the results
inspect(sort(rules,by="lift")[1:10])
dataset<-read.csv("Market_Basket_Optimisation.csv",header = FALSE)
View(dataset)
source('~/Desktop/Machine Learning/ML in Python and R/Association Rule Learning-Apriori/Apriori.R')
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Reinforcement Learning/UCB")
