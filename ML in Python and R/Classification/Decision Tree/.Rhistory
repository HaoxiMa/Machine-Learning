Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释随机森林的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(randomForest)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
set.seed(1234)
classifier<-randomForest(x = training_set[,-3],y = training_set$Purchased,ntree=10)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
cm
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Random Forest")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释随机森林的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(randomForest)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
set.seed(1234)
classifier<-randomForest(x = training_set[,-3],y = training_set$Purchased,ntree=10)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
cm
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Random Forest")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释随机森林的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(randomForest)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
classifier<-randomForest(x = training_set[,-3],y = training_set$Purchased,ntree=10)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
cm
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Random Forest")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释随机森林的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(randomForest)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
classifier<-randomForest(x = training_set[,-3],y = training_set$Purchased,ntree=10)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
cm
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Random Forest")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释随机森林的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(randomForest)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
classifier<-randomForest(x = training_set[,-3],y = training_set$Purchased,ntree=10)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
cm
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Random Forest")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释随机森林的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(randomForest)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
classifier<-randomForest(x = training_set[,-3],y = training_set$Purchased,ntree=10)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
cm
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Random Forest")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释随机森林的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(randomForest)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
classifier<-randomForest(x = training_set[,-3],y = training_set$Purchased,ntree=10)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
cm
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Random Forest")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释随机森林的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(randomForest)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
classifier<-randomForest(x = training_set[,-3],y = training_set$Purchased,ntree=10)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
cm
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Random Forest")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释随机森林的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(randomForest)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
classifier<-randomForest(x = training_set[,-3],y = training_set$Purchased,ntree=10)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
cm
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Random Forest")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释随机森林的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(randomForest)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
classifier<-randomForest(x = training_set[,-3],y = training_set$Purchased,ntree=10)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
cm
#Visualizing the training set results
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("Age","EstimatedSalary")
y_grid<-predict(classifier,grid_set,type = "class")
plot(set[,-3],main="CLassifier (Training set)",xlab="Age",ylab="EstimatedSalary",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
#比起python，并没有过度拟合
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.001)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.001)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("Age","EstimatedSalary")
y_grid<-predict(classifier,grid_set,type = "class")
plot(set[,-3],main="CLassifier (Training set)",xlab="Age",ylab="EstimatedSalary",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
#比起python，并没有过度拟合
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("Age","EstimatedSalary")
y_grid<-predict(classifier,grid_set,type = "class")
plot(set[,-3],main="CLassifier (Training set)",xlab="Age",ylab="EstimatedSalary",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
#比起python，并没有过度拟合
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Random Forest")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离,我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Random Forest to training set
library(randomForest)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
set.seed(1234)
classifier<-randomForest(x = training_set[,-3],y = training_set[,3],ntree=10)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3])
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Visualizing the training set results
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("Age","EstimatedSalary")
y_grid<-predict(classifier,grid_set,type = "class")
plot(set[,-3],main="CLassifier (Training set)",xlab="Age",ylab="EstimatedSalary",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
#从图中看，存在一定的过度拟合
setwd("/Users/mahaoxi/Desktop/Machine Learning/ML in Python and R/Classification/Decision Tree")
dataset<-read.csv("Social_Network_Ads.csv")
dataset<-dataset[,3:5]
library(caTools)
set.seed(123)
Split<-sample.split(dataset$Purchased,SplitRatio = 0.75)
training_set<-subset(dataset,Split == TRUE)
test_set<-subset(dataset,Split ==FALSE)
#算法没有用到距离，同时为了能更好更直观的解释决策树的分类标准，我们不需要做feature scaling
#但这里我们为了画图，就做一下feature scaling
maxs <- apply(training_set[,c(1,2)], 2, max)
mins <- apply(training_set[,c(1,2)], 2, min)
training_set[,c(1,2)]<- scale(training_set[,c(1,2)], center = mins, scale = maxs - mins)
test_set[,c(1,2)]<- scale(test_set[,c(1,2)], center =  mins, scale = maxs - mins)
#Fitting Decision Tree to training set
library(rpart)
#和Naive Bayes一样需要手动将response转换成分类数据
str(training_set)
training_set$Purchased<-as.factor(training_set$Purchased)
classifier<-rpart(Purchased~., data=training_set)
#在不做feature scaling时，画出决策树
#plot(classifier)
#text(classifier)
#Predicting the test set results
y_pred<-predict(classifier,test_set[,-3],type = "class")
#Making confusion matrix
cm <-table(test_set$Purchased,y_pred)
#Visualizing the training set results
set<-training_set
X1<-seq(min(set[,1])-0.1, max(set[,1])+0.1, 0.01)
X2<-seq(min(set[,2])-0.1, max(set[,2])+0.1, 0.01)
grid_set<-expand.grid(X1,X2)
names(grid_set)<-c("Age","EstimatedSalary")
y_grid<-predict(classifier,grid_set,type = "class")
plot(set[,-3],main="CLassifier (Training set)",xlab="Age",ylab="EstimatedSalary",xlim=range(X1),ylim=range(X2))
contour(X1,X2,matrix(as.numeric(y_grid),length(X1),length(X2)),add=TRUE)
points(grid_set,col=ifelse(y_grid==1,"red","green"))
points(set[,-3],pch=21,bg=ifelse(set[,3]==1,"orange","blue"))
#比起python，并没有过度拟合
library(CASdatasets)
install.packages("CASdatasets", repos = "http://cas.uqam.ca/pub/R/", type="source")
install.packages("CASdatasets", repos = "http://cas.uqam.ca/pub/R/", type="source")
library(CASdatasets)
data(credit)
summary(credit)
names(credit)
attach(credit)
dim(credit)
head(credit)
predictors <- names(credit)[-5]
log_credit <- log(credit_amount)
credit2 <- data.frame(log_credit,credit[predictors])
# no NA data to remove
names(credit2)
head(credit2)
# break data into training (80%) and test set (20%)
set.seed(1)
vInd <- sample(1:nrow(credit2), size=nrow(credit2)*.80)
credit.train <- credit2[vInd,]
credit.test <- credit2[-vInd,]
# ggplot
library(ggplot2)
library(viridis)
hist(log_credit,br=40,data=credit.train)
ggplot(credit.train, aes(x=duration, y=age)) +
geom_point(aes(color=log_credit)) +
scale_color_viridis(option="D") +
xlab("duration (months)") + ylab("age (years)") +
theme(text = element_text(size=14),legend.position="bottom")
library(rpart)
library(rpart.plot)
tree1 <- rpart(log_credit ~ ., data=credit.train, method="anova", control=rpart.control(cp=0))
summary(tree1)
names(summary(tree1))
# plot the decision tree
par(mar=c(1,1,1,1))
plot(tree1, uniform=T, margin=0.05)
text(tree1, use.n = TRUE)
title("This is a decision tree using all features")
tree1$variable.importance
# perform cross validation
printcp(tree1)
plotcp(tree1)
prune.tree1 <- rpart(log_credit ~ ., data=credit.train, method="anova", control=rpart.control(cp=0.016))
summary(prune.tree1)
summary(prune.tree1)
# plot the pruned tree
par(mar=c(1,1,1,1))
plot(prune.tree1, uniform=T, margin=0.05)
text(prune.tree1, use.n = TRUE)
title("This is the pruned decision tree")
summary(tree1)
names(summary(tree1))
par(mar=c(1,1,1,1))
plot(tree1, uniform=T, margin=0.05)
text(tree1, use.n = TRUE)
title("This is a decision tree using all features")
prune.tree1 <- rpart(log_credit ~ ., data=credit.train, method="anova", control=rpart.control(cp=0.016))
summary(prune.tree1)
par(mar=c(1,1,1,1))
plot(prune.tree1, uniform=T, margin=0.05)
text(prune.tree1, use.n = TRUE)
title("This is the pruned decision tree")
library(rpart.plot)
prp(prune.tree1)
title("This is the pruned decision tree")
install.packages("rpart.plot")
# how to make the picture better
library(rpart.plot)
prp(prune.tree1)
title("This is the pruned decision tree")
library(rattle)
install.packages("rattle")
fancyRpartPlot(prune.tree1,sub="This is the pruned decision tree")
library(rattle)
fancyRpartPlot(prune.tree1,sub="This is the pruned decision tree")
prp(prune.tree1)
title("This is the pruned decision tree")
fancyRpartPlot(prune.tree1,sub="This is the pruned decision tree")
# variable importance is measured according to how much reduction in the impurity
# contributed by the variable
prune.tree1summary <- summary(tree1)
vImp <- prune.tree1summary$variable.importance
vImp
vImp <- vImp * 100 / max(vImp)
ind <- order(vImp)
par(las=2) # make label text perpendicular to axis
par(mar=c(3,10,1,1)) # increase y-axis margin.
barplot(vImp[ind], main="", horiz=TRUE, names.arg=names(vImp[ind]))
title("Variable importance")
y <- credit.test$log_credit
ybar <- mean(credit.test$log_credit)
SST <- sum( (y-ybar)^2 )
yhat_tree1 <- predict(prune.tree1, credit.test)
1 - sum( (yhat_tree1 - y)^2) / SST
# see how good the fit of the trees are
plot(credit.test$log_credit, yhat_tree1, xlab="log(credit amount)", ylab="Regression Tree - pruned")
abline(0,1)
# ensemble methods
# prediction using gradient boosting
# gradient boosting
library(gbm)
install.packages("gbm")
# ensemble methods
# prediction using gradient boosting
# gradient boosting
library(gbm)
library(caret)
set.seed(1)
boost1 <- gbm(log_credit ~ ., data=credit.train, distribution="gaussian", n.trees=5000, interaction.depth=3, importance="impurity")
summary(boost1)
names(boost1)
boost1 <- gbm(log_credit ~ ., data=credit.train, distribution="gaussian", n.trees=5000, interaction.depth=3, importance="impurity")
library(randomForest)
set.seed(1)
bag1 <- randomForest(formula=log_credit ~ ., data=credit.train, mtry=20, importance=TRUE)
bag1
summary(bag1)
names(bag1
）
names(bag1)
yhat_bag1 <- predict(bag1, credit.test)
dev.new(width=4, height=4)
par(mar=c(4,4,2,1))
plot(credit.test$log_credit, yhat_bag1, xlab="log(credit amount)", ylab="Bagged Tree model",pch=20)
abline(0,1)
plot(credit.test$log_credit, yhat_bag1, xlab="log(credit amount)", ylab="Bagged Tree model",pch=20)
library(randomForest)
set.seed(1)
bag1 <- randomForest(formula=log_credit ~ ., data=credit.train, mtry=20, importance=TRUE)
bag1
summary(bag1)
names(bag1)
yhat_bag1 <- predict(bag1, credit.test)
dev.new(width=4, height=4)
par(mar=c(4,4,2,1))
plot(credit.test$log_credit, yhat_bag1, xlab="log(credit amount)", ylab="Bagged Tree model",pch=20)
abline(0,1)
set.seed(1)
bag2 <- randomForest(formula=log_credit ~ ., data=credit.train, mtry=4, importance=TRUE)
bag2
yhat_bag2 <- predict(bag2, credit.test)
par(mar=c(4,4,1,1))
plot(credit.test$log_credit, yhat_bag2, xlab="log(credit amount)", ylab="Random Forest",pch=20)
abline(0,1)
# variable importance
bag2Imp <- bag2$importance[,2]
varImp <- names(bag2Imp)
bag2Imp <- bag2Imp * 100 / max(bag2Imp)
ind <- order(bag2Imp)
par(las=2) # make label text perpendicular to axis
par(mar=c(3,10,1,1)) # increase y-axis margin.
barplot(bag2Imp[ind], main="", horiz=TRUE, names.arg=varImp[ind])
title("Variable importance")
